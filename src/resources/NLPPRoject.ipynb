{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a9278c",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e261f2ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22947, 2) (12965, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Pleasant 10 min walk along the sea front to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Really lovely hotel. Stayed on the very top fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Ett mycket bra hotell. Det som drog ner betyge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>We stayed here for four nights in October. The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>We stayed here for four nights in October. The...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                               Text\n",
       "0          1  Pleasant 10 min walk along the sea front to th...\n",
       "1          1  Really lovely hotel. Stayed on the very top fl...\n",
       "2          1  Ett mycket bra hotell. Det som drog ner betyge...\n",
       "3          1  We stayed here for four nights in October. The...\n",
       "4          1  We stayed here for four nights in October. The..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import standard library for preprocessing\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import scipy\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('Hotel_Reviews_Compact.csv') # Big dataset\n",
    "# dataset = pd.read_csv('Datafiniti_Hotel_Reviews_Jun19.csv') # Middle\n",
    "# dataset = pd.read_csv('Datafiniti_Hotel_Reviews.csv') # Middle dataset\n",
    "\n",
    "# Get rid of unsed columns and change into readable comfortable manner\n",
    "dataset = dataset.drop(columns = ['address', 'categories', 'city', 'country', 'latitude', 'longitude', 'name', 'postalCode', 'province', 'reviews.date', 'reviews.dateAdded', 'reviews.doRecommend', 'reviews.id', 'reviews.title', 'reviews.userCity', 'reviews.username', 'reviews.userProvince'])\n",
    "dataset = dataset.rename(columns = {'reviews.rating': 'Sentiment', 'reviews.text': 'Text'})\n",
    "\n",
    "# If the rating is equal to 3 or more it is a good review, otherwise bad\n",
    "dataset['Sentiment'] = dataset['Sentiment'].map(lambda x: 1 if x > 3.0 else 0)\n",
    "positive = dataset[dataset['Sentiment'] == 1]\n",
    "negative = dataset[dataset['Sentiment'] == 0]\n",
    "print(positive.shape, negative.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73bf62d",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51b5b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "stemming = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords_in_regex = ' | '.join(stopwords)\n",
    "\n",
    "# Params: String\n",
    "# Return: String\n",
    "def remove_stopwords(string):\n",
    "    # Get all stopwords in English language\n",
    "    stopwords_in_regex = ' | '.join(stopwords)\n",
    "    string = re.sub(rf'({stopwords_in_regex})', ' ', string)\n",
    "    return string\n",
    "\n",
    "# Params: String\n",
    "# Returns: Tokens (array of string)\n",
    "def tokenize_string(string):\n",
    "    tokens = nltk.word_tokenize(string)\n",
    "    return tokens\n",
    "\n",
    "# Params: Tokenized string\n",
    "# Returns: String\n",
    "def stemm_string(tokens):\n",
    "    stemmed_tokens = []\n",
    "    for token in tokens:\n",
    "        stemmed_tokens.append(stemming.stem(token))\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "# Params: Tokenized string\n",
    "# Returns: String\n",
    "def lemmatize_string(tokens):\n",
    "    lemmatized_tokens = []\n",
    "    for token in tokens:\n",
    "        lemmatized_tokens.append(wnl.lemmatize(token))\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Params: Tokenizes string\n",
    "# Returns: String\n",
    "def stemm_lemmatize_string(string):\n",
    "  string = re.sub(rf'({stopwords_in_regex})', ' ', string)\n",
    "  tokenized_string = tokenize_string(string)\n",
    "  stemm_lemm_tokens = []\n",
    "  for token in tokenized_string:\n",
    "    stemm_token = stemming.stem(token)\n",
    "    lemm_stemm_token = wnl.lemmatize(stemm_token)\n",
    "    stemm_lemm_tokens.append(lemm_stemm_token)\n",
    "  return ' '.join(stemm_lemm_tokens)\n",
    "\n",
    "# Params: String\n",
    "# Return: String\n",
    "def preprocess_string(row):\n",
    "\n",
    "    # Lowers the uppercase characters\n",
    "    row = str(row).lower()\n",
    "\n",
    "    # In string removes characters inside '<' and '>' including '<' and '>'\n",
    "    row = re.sub(r'\\<[^<>]*\\>', '', row)\n",
    "\n",
    "    # Removes all characters that is not from A-Z and a-z and occures 1 or more times (removes punctuations)\n",
    "    row = re.sub(r'[^A-Za-z ]+', '', row)\n",
    "    \n",
    "    # Remove double spaces\n",
    "    row = re.sub(r'  *', ' ', row)\n",
    "\n",
    "\n",
    "    # Stem and Lemm string\n",
    "    row = stemm_lemmatize_string(row)\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00da4164",
   "metadata": {},
   "source": [
    "## Preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82bc8dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Text'] = dataset['Text'].map(lambda x: preprocess_string(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd9b0355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>pleasant min walk along sea front the water bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>realli love hotel stay the top floor were surp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ett mycket bra hotel det som drog ner betyget ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>we stay for four night octob hotel staff welco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>we stay for four night octob hotel staff welco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                               Text\n",
       "0          1  pleasant min walk along sea front the water bu...\n",
       "1          1  realli love hotel stay the top floor were surp...\n",
       "2          1  ett mycket bra hotel det som drog ner betyget ...\n",
       "3          1  we stay for four night octob hotel staff welco...\n",
       "4          1  we stay for four night octob hotel staff welco..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc849be",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c083caaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28729,) (7183,) (28729,) (7183,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(dataset['Text'], dataset['Sentiment'], test_size = 0.2)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73bc049",
   "metadata": {},
   "source": [
    "## Defining the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b4c5f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
       "                ('classifier', SVC(kernel='linear'))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "# Logistic regression\n",
    "lgr = LogisticRegression(max_iter=1100)\n",
    "\n",
    "model = Pipeline([('vectorizer', tfidf_vec), ('classifier', lgr)])\n",
    "\n",
    "# SVM with kernel\n",
    "svm_lin = svm.SVC(kernel='linear')\n",
    "model_svm = Pipeline([('vectorizer', tfidf_vec), ('classifier', svm_lin)])\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "model_svm.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329a503a",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdfa16ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: [1 0]\n",
      "SVM: [1 0]\n"
     ]
    }
   ],
   "source": [
    "example_text = [\"Lovely clean room, breakfast was absolutely fantastic and everyone was really friendly\",\n",
    "               \"The hotels treat you like garbage if you book through booking.com. I've booked through them a few times and have only recently just realised I get downgraded on stuff with the hotel if my reservation is through booking.com. The crappiest rooms, no parking available, rooms not being serviced, amenities a disappointment.. The list goes on.. Plus you pay more!! Next time I'll just book through the hotel itself. It's cheaper and you get treated better\"] \n",
    "# from booking.com\n",
    "example_result = model.predict(example_text)\n",
    "print(f\"Logistic Regression: {example_result}\")\n",
    "\n",
    "example_result = model_svm.predict(example_text)\n",
    "print(f\"SVM: {example_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848239d2",
   "metadata": {},
   "source": [
    "## Result printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d701be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(inp_model):\n",
    "\tlast_words = inp_model.predict(x_test)\n",
    "\tconf_matrix = confusion_matrix(last_words, y_test)\n",
    "\ta_score = accuracy_score(last_words, y_test)\n",
    "\tpre_score = precision_score(last_words, y_test, average = 'weighted')\n",
    "\trec_score = recall_score(last_words, y_test, average = 'weighted')\n",
    "\tf1_score = 2 * (pre_score * rec_score) / (pre_score + rec_score)\n",
    "\tprint(\"Confusion matrix: \", conf_matrix)\n",
    "\tprint(\"Accuracy : \", a_score)\n",
    "\tprint(\"Precision : \", pre_score)\n",
    "\tprint(\"Recall : \", rec_score)\n",
    "\tprint(\"F1 : \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67ad1c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Confusion matrix:  [[1789  461]\n",
      " [ 850 4083]]\n",
      "Accuracy :  0.8174857301962968\n",
      "Precision :  0.8294345904277571\n",
      "Recall :  0.8174857301962968\n",
      "F1 :  0.8234168141771099\n",
      "SVM:\n",
      "Confusion matrix:  [[1819  497]\n",
      " [ 820 4047]]\n",
      "Accuracy :  0.8166504246136712\n",
      "Precision :  0.8257045314154083\n",
      "Recall :  0.8166504246136712\n",
      "F1 :  0.8211525209095983\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression:\")\n",
    "get_results(model)\n",
    "\n",
    "print(\"SVM:\")\n",
    "get_results(model_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41994303",
   "metadata": {},
   "source": [
    "## Save the model to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2445962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_file(filename, model):\n",
    "\tpickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "save_file('log_trained.sav', model)\n",
    "save_file('svm_trained.sav', model_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85915c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
